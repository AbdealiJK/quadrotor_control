%\documentclass[PhD]{iitmdiss}
%\documentclass[MS]{iitmdiss}
%\documentclass[MTech]{iitmdiss}
\documentclass[BTech]{iitmdiss}
\usepackage{times}
\usepackage{t1enc}

\usepackage{graphicx}
\usepackage{epstopdf}
\usepackage{hyperref} % hyperlinks for references.
\usepackage{amsmath} % easier math formulae, align, subequations \ldots
\usepackage{xcolor}
\usepackage{amsfonts}

\newcommand\todo[1]{\textcolor{red}{{\bf TODO}: #1}}

\begin{document}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Title page

\title{Autonomous control of Quad-rotor payloads}

\author{Abdeali JK}

\date{April 2016}
\department{PHYSICS}

%\nocite{*}
\maketitle

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Certificate
\certificate

\vspace*{0.5in}

\noindent This is to certify that this thesis, submitted by {\bf Abdeali JK and Kunal Grover}, to the Indian Institute of Technology, Madras, for the award of the degree of {\bf B. Tech.}, is a bona fide record of the research work done by them under our supervision. The contents of this thesis, in full or in parts, have not been submitted to any other Institute or University for the award of any degree or diploma.

\vspace*{1.5in}

\begin{singlespacing}

\begin{minipage}[t]{0.45\textwidth}
  {\bf Prof. Balaraman Ravindran} \\
  Research Guide \\
  Associate Professor \\
  Dept. of Computer Science \\
  and Engineering \\
  IIT-Madras, 600 036
\end{minipage}
\hfill
\begin{minipage}[t]{0.45\textwidth}
  {\bf Prof. Ramkrishna Pasumarthy} \\
  Research Guide \\
  Assistant Professor \\
  Dept. of Electrical Engineering \\
  IIT-Madras, 600 036
\end{minipage}

\end{singlespacing}

\vspace*{0.25in}
\noindent Place: Chennai\\
Date: \todo{4 April 2016}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Acknowledgements
\acknowledgements

We would like to thank the Indian Institute of Technology, Madras for giving us the opportunity to work on a project and our guide Dr. B. Ravindran, for allowing us to work with him and his invaluable help throughout the project, right from the identification of our project topic, to helping mould approaches and identifying new directions to work in when a line of thought had to be abandoned.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Abstract

\abstract

\noindent KEYWORDS: \hspace*{0.5em} \parbox[t]{4.4in}{Quad-rotor Control; Reinforcement Learning; Policy Search; Payload control; PEGASUS.}

\vspace*{24pt}

\noindent Autonomous quad-rotor flight is a problem which is challenging because of complex dynamics and noise. In this thesis, we describe a method to control quad-rotors by applying policy search from reinforcement learning. We first set-up a simulated quad-rotor for testing purposes using gazebo and ROS. We then use the simulated model to learn basic control like hovering in place, and then moving in linear paths. Once a stable basic control is achieved the quad-rotor learns to move in various trajectories and paths. Finally, a payload is attached to the quad-rotor and we learn to move the payload in a specified trajectory.

\pagebreak

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Table of contents etc.

\begin{singlespace}
\tableofcontents
\thispagestyle{empty}

\listoftables
\addcontentsline{toc}{chapter}{LIST OF TABLES}
\listoffigures
\addcontentsline{toc}{chapter}{LIST OF FIGURES}
\end{singlespace}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Abbreviations
% \abbreviations

% \noindent
% \begin{tabbing}
% xxxxxxxxxxx \= xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \kill
% \textbf{IITM}   \> Indian Institute of Technology, Madras \\
% \textbf{RTFM} \> Read the Fine Manual \\
% \end{tabbing}

% \pagebreak

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Notation

% \chapter*{\centerline{NOTATION}}
% \addcontentsline{toc}{chapter}{NOTATION}

% \begin{singlespace}
% \begin{tabbing}
% xxxxxxxxxxx \= xxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxxx \kill
% \textbf{$r$}  \> Radius, $m$ \\
% \textbf{$\alpha$}  \> Angle of thesis in degrees \\
% \textbf{$\beta$}   \> Flight path in degrees \\
% \end{tabbing}
% \end{singlespace}

% \pagebreak
\clearpage

% The main text will follow from this point so set the page numbering
% to arabic from here on.
\pagenumbering{arabic}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Introduction.

\chapter{Introduction}

\section{Motivation}

In robotics, there are many cases where the problem to solve is not known, is stochastic, has non-linear dynamics and has a large number of actions and states. In such cases, traditional control methods which require complete knowledge of the system may be difficult to model. Reinforcement learning is a method by which the robot does not need much knowledge about the environment, and can be used in such cases.

Quad-rotors (or drones) are becoming common with commercial quad-rotors available in the market and are popular among hobbyists. Most commercial quad-rotors are not certified to be operated outside line of sight, let alone autonomously. The major issue for this is the unexpected noise from the environment which cannot be controlled in a real life environment. There has been a lot of research in this domain, and autonomous control of quad-rotors is becoming a reality.

One of the major use cases for an autonomous quad-rotor is to carry payloads from one place to another. This could be used for something as simple as delivery or for reconnaissance. The payloads adds a new complexity and constraints to the earlier system. For example, a payload shouldn't hit obstacles on the course. And if the payload is attached using a non-rigid joint to the quad-rotor, the dynamics become even more complicated.

As a reinforcement problem, this brings up further questions. In the original scenario, it was already difficult to define the reward as a "good flight" is not clearly defined. Now, we want to define a "good flight for the payload" also.

\begin{figure}[h]
  \centering
    \includegraphics[width=0.5\textwidth]{quadrotor.jpg}
    \caption{Picture of a quadrotor flying.}
\end{figure}

\section{Outline}

It is difficult to work with quad-rotors without good simulators. In reinforcement learning, where the quad-rotor learns the bad states and good states, collisions, flips, and unstable states are unavoidable. A real quad-rotor would break within a few trials of this learning procedure.

Hence, the first step is to set-up a simulated environment where the quad-rotor can learn. Ideally, the simulated quad-rotor is as close to real as possible so that the learning can be directly transferred. We used the gazebo simulator with ROS to emulate this. This enables us to perform simulate wind, turbulence, motors, and even joints.

Next, we need an algorithm which would be suitable for our specific use case. Here, we chose to use PEGASUS - a policy search method. It is capable of handling the stochastic nature of the environment and the continuous action and state space well. It has already been used to control helicopters and perform stunts with them.

We initially use PEGASUS on the quad-rotor without any payload. This is a sanity check to ensure that PEGASUS can indeed handle the system. Then, we add the payload and learn a policy that can control the payload directly.

\section{The Reinforcement Learning Problem}

\section{Markov Decision Process}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Literature Review}

\section{Reinforcement learning in control}

There has been a lot of work done about control. The review \cite{ControlForQuads} by Andrew Zulu and Samuel John gives an overview of techniques that have been formulated. It speaks of techniques that merge control theory and machine learning. For example, fuzzy logic, neural network based controls, and genetic algorithms. Fuzzy logic based controls to fuzzify control theory algorithms are frequently used. For example, Fuzzy logic to identify the constants required in PID and LQR were researched by E. Abbasi and J. Mahjoob.

Reinforcement learning has been applied to control problems frequently. Reinforcement learning has been successful in all sorts of applications like marketing strategy selection and advertisement targeting, cell-phone network routing and multiplexing, bipedal to octapedal locomotion, and efficient web-page indexing.

Value iteration and Policy iteration are the core concepts of control using reinforcement learning. In value iteration, the value function is updated in every iteration trying to update the estimated value using the Bellman equations. It can be shown that the values converge to the actual value of the state over time. Having found the value function of the system, we can create a policy. In policy iteration, the value function of the current policy is computed and then the policy is updated using the current value function.

Policy search emerged as an alternative method to policy iterations or value iterations, which did not need to calculate the value function at all. This involved creating a set of policies that the objective can be achieved by and searching in this set for the optimal policy. Although it seems to be a difficult task which borders of brute force, techniques have been formulated to smartly converge to the optimal policy quickly.

Policy search methods became popular due to the fact that the value function was not directly involved in the computation, and variations that use model free policy search and model based policy search were created. Model free methods include techniques like Policy gradients, Expectation maximization, and Stochastic search methods. As no model is assumed in these cases, they were slow but extremely generalized. Model based methods like Analytic policy gradients and Trajectory optimization estimate the model in every iteration and update a policy based on this estimated model.

To optimize on data used, step based policy search was conceptualized and used. These attempt to evaluate the quality of a state, action pair using the reward that the pair generates rather than the parameters defining a policy. Step based policy search methods due to their nature of formulation are not favourable for smooth trajectories but provide uncorrelated exploration unlike episode based search. Examples of step based policy search include Reinforce, Policy gradient theorem, and Episodic natural Actor-Critic. Examples of episode based policy search are Episodic REPS, Cross-Entropy search, and PEGASUS.

An episode based policy search method is well suited for our task. It fits easily into dynamic control tasks as the exploration is rarely uncorrelated in such cases. Also, the trajectories created from this are smooth - which is normally the case for robotic systems like an arm or a UAV. Hence, step-based policy search becomes complicated and gives un-natural solutions for a robotic task like autonomous flight.

\section{Continuous states and actions}

A dynamic control task like ours is a typical example of a continuous state and action variables. Many existing Reinforcement Learning techniques assume discrete sets for states and actions. Because of this continuous variables are frequently binned (mapped to discrete sets) to use these techniques. If the binning is done with very small bin widths, there is no concept of a generalized policy and over-fitting takes place. While, if the bin widths are too large hidden variables are created. A multi-dimensional state space complicates things even more.

A more reasonable approach is to use function approximators. Function approximation provides an elegant solution to multi-dimensional spaces. And in most cases, using a function approximation reduces the space to a smaller subspace. And optimality conditions within this subspace still hold true. Hence, to achieve an optimal solution, even function approximation needs to be carried out carefully.

In episode based policy search methods where the policy is directly found, it becomes even simpler. The policy itself can be function approximated as a function that takes in a continuous state and gives out a continuous action. Here, we only need to identify a single function basis for the approximation.

In such cases, the policies that are searched (the policy class) are restricted due to this parametrization. If the function approximation and parametrization is not done correctly, the optimal policy may not lie in the class at all. In such cases, the result got from a typical policy search algorithm is simply the optimal solution in this subspace defined by the policy class.

\begin{figure}[h]
  \centering
    \includegraphics[width=0.75\textwidth]{quadrotor_states_actions.png}
    \caption{Quadrotor schematic with states and actions depicted. All states and actions are continuous.}
\end{figure}

\section{Payload control for UAVs}

There has been some work to control the payload of an UAV directly using Control Theory \cite{PayloadControlTheory,PayloadControlTheory2}. These approaches again require a lot of knowledge about the system, including the tension in the joint. Some even approximate the system to be linear to be able to solve the resulting differential equations.

There has also been some work in controlling payloads of UAVs using Reinforcement Learning \cite{PayloadLSPI}. In this, Least Square Policy Iteration (LSPI) is used. Three different LSPI are used to track the x, y, and z dimensions. This has the disadvantage of these three learning policy iterations that can clash with each other. In the case of a quad-rotor, where the system is under actuated, the x and y dimensions are correlated.

\begin{figure}[h]
  \centering
    \includegraphics[width=0.5\textwidth]{payload.png}
    \caption{Quadrotor with a payload attached using a non rigid body. States (cartesian co-ordinates and euler angles) of the payload are shown.}
\end{figure}

\section{Apprenticeship learning}

In reinforcement learning we consider a reward function, which describes the task and the target that needs to be achieved. A dynamics model is implicit which describes how the agent and interaction with the environment and how the environment reacts to the action from the agent. Then we try to find an optimal policy that maximizes the expected return.

Reinforcement learning can be applied to tasks as ill defined as "drive the car well". This is not clear and has many many inherent meanings like "drive to the destination quickly", "do not crash into anything", "stop at red signals" and even more subtle meanings like "change lanes as little as possible", "move in smooth trajectories" and so on. In such cases, which are not clearly defined it is even more difficult to quantify the expected task and create a reward function.

Apprenticeship learning \cite{ApprenticeshipLearning} helps to solve this issue by providing  learning a task from a mentor or teacher which already knows the target. The mentor is asked to perform the task and the trajectories and actions used by the mentor is stored in the memory of the agent. Then the agent uses this memory in some way to help itself to find a policy suitable for the task.

The most common method of using the mentors information in the agent or the apprentice is to make a reward function based on the trajectories. The reward function can be derived by identifying which transitions occur frequently in the mentor's trajectories and which ones do not. The frequent transitions should give a larger reward while the transitions that do not occur give no or negative rewards. The mentor can even specify which trajectories are bad and should not occur for a more balanced reward function.

In the case of the continuous rewards and states like ours, things are not as easy. Here, it is impossible to count transitions and so the above method is not possible. In such cases the trajectories need to be time normalized and then stitched to find the optimal trajectory for the task.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{PEGASUS}

PEGASUS \cite{PEGASUS} (Policy Evaluation-of-Goodness And Search Using Scenarios) is a technique devised by Andrew N. G. and Michael Jordan which finds a policy by searching through the whole policy space. It eliminates policies based on the "goodness" of it. Using the goodness quantification, it then attempts to find the next policy to check.

\section{Value of the policy}

In Reinforcement Learning we already have the notion of a policy as well as a value function. The policy is any function denoted by $\pi$ mapping:
\begin{equation} \begin{split}
  \pi :& \quad \mathbb{S} \rightarrow \mathbb{A} \qquad or \\
  \pi :& \quad \mathbb{S} \times \mathbb{A} \rightarrow [0,1]
\end{split} \end{equation}

The value function of a policy is then defined as:
\begin{equation}
  V^{\pi} : \quad \mathbb{S} \rightarrow \mathbb{R}
\end{equation}

Here, we define the value of the policy or the "goodness" of the policy as the value of the initial state ($s_{0}$) in an episode. This can also be thought of the total return the policy is capable of getting. Extending it to multiple episodes, we can take the expectation over the distribution of initial states $\mathbb{D}$:
\begin{equation}
  V(\pi) = \mathbf{E}_{s_{0} \sim \mathbb{D}} [ V^{\pi} (s_{0})]
\end{equation}

As we don't really know the value of the policy before hand, to compute the value of the policy, we need to find the rewards for an episode and compute the return. We run this multiple times so that we can average out the effect of $\mathbb{D}$ - The distribution of initial states. If we run $m$ episodes to reduce the variance in the value of the policy, we can write:
\begin{equation}
  V(\pi) \approx \hat{V}(\pi) = \frac{1}{m} \sum_{i=1}^{m} V^{\pi} (s_{0}^{i})
\end{equation}

\section{Optimal policy}

Now that we have a quantitative scalar for a policy, we can compare policies using it. So a policy $\pi_{1}$ is considered better than another policy $\pi_{2}$ if $V(\pi_{1}) > V(\pi_{2})$. Hence, we can now define an optimal policy as:
\begin{equation}
  \pi^{*} = {\arg \max}_{\pi \forall \mathbf{\Pi}} {V(\pi)}
\end{equation}

In a problem where the actions are discrete and an episode terminates after $H$ steps, a policy is simply a sequence of $H$ actions from the action space. As the actions space is discrete, there will be a discrete set of policies. To be specific, there will be $|\mathbb{A}|^{H}$ policies at maximum. Here, if we find the value of the policies, we can compare policies and eventually find the optimal policy.

However, in a problem where the actions are continuous it is not feasible to find the values of all the policies. In such a case, we use function approximation and use a parametric function for the policy. This parametric function can be anything, as long as it is continuous and can be defined by finite parameters ($\mu$). Now, we can write the value of the policy as:
\begin{equation}
  V(\pi) = V(\pi_{\mu})
\end{equation}

Hence, the value now becomes a function of a finite set of continuous variables $\mu$. We can now imagine a $|\mu|+1$ dimensional graph which maps $\mu$ to $V$. In this multi-dimensional graph, we can use any method to find the maximum of a function. For example, gradient descent, stochastic gradient descent, BFGS algorithm, random search, etc. Using these techniques we can find the maximum $V$ and hence the optimal policy $\pi^{*}$.

\begin{figure}[h]
  \centering
    \includegraphics[width=\textwidth]{gradient_descent.png}
    \caption{An example of a policy with 2 parameters. The value of the policy at these parameters has been plot. The optimal policy is marked at the maximum value. The maximum valuye can be found using maximization techniques like gradient descent, expectation maximization, BFGS, etc.}
\end{figure}

\section{Stochastic MDP to non-stochastic POMDP}

The PEGASUS algorithm works for both MDPs and POMDPs. But, it involves running a specific scenario multiple times - and assumes the same transition function for the environment. If the transition function is not same, the value of the policy will have a large variance. Hence, it does not work well with stochastic transition functions.

This can be tackled by simply converting stochastic MDPs or POMDPs into an appropriate POMDP with a random seed that is not observed. Hence we're creating a new MDP with the states from the original MDP along with the random variables that will be used by the transition function. This potentially makes the new POMDP's state space infinite dimensional.

Let us take the original stochastic (PO)MDP as $MDP_{1}$ which has the state space $\mathbb{S}$. Now, we get an infinite set of random numbers generated from a true uniform random generator $[0,1]^{\infty}$. We now create a new POMDP $MDP_{2}$ which has the state space $\mathbb{S} \times [0,1]^{\infty}$. This new POMDP $MDP_{2}$ has a environment that uses these uniformly random variables in it's state for any non-stochastic calculations it has. It is trivial to convert a uniform random variable into any other distribution like Bernouilli, Binomial or others as required. Hence, the transition function in $MDP_{2}$ is deterministic based on it's state space.

For implementation purposes, it is impossible to use state spaces with infinite dimensions, and this is not really needed. We already truncate our episodes after $H$ steps, so we need to only store random variables that would be used within $H$ steps. This reduces the state space to $\mathbb{S} \times [0,1]^{|H|}$. To reduce the memory complexity furthermore, only a seed needs to be added to the state space. This seed can initiate an appropriate random number generator for an episode.

\section{Application to helicopters}

PEGASUS has already been used in helicopters \cite{HelicopterPegasus}  by Andrew Ng, Jin Kim, Michael Jordan and Shankar Sastry. In their paper they describe using PEGASUS on a model they created of the helicopter which was used to learn. After the learning on this model, they implemented the policy learnt on the actual helicopter and obtained successful results.

There, they created the Model of the helicopter by getting trajectories from a human pilot. This is not an ideal method as the human pilot is incapable of using the helicopter in certain states and the transition function would be biased to better scenarios rather than worse ones. This introduces a bit of the model of the helicopter into the model inherently.

Next, they design a shallow neural network to make the policy class $\Pi$ that PEGASUS requires. The weights of the neural network are taken as the parameters of the policy that are learnt by PEGASUS. This neural network is specific for the helicopter and cannot be generalized to other UAVs with different dynamics.

The reward function used in this paper is the L2 norm of the difference between the expected position and the original position. It includes the x, y, z position as well as the yaw. And the linear velocities are included in this too, with the target velocity being 0. As this is specifically for hovering, they also have a negative reward for large actions, as jerky behaviour is not wanted. With this system, they were able to make the quad-rotor hover and move in lines successfully.

\begin{figure}[h]
  \centering
    \includegraphics[width=0.4\textwidth]{pegasus_helicopter.png}
    \caption{Control of a helicopter using PEGASUS by Andrew Ng, Jin Kim, Michael Jordan and Shankar Sastry \cite{HelicopterPegasus}. A comparison of maneuver diagrams from RC helicopter competition\protect\footnotemark compared with actual trajectories from the model learnt by PEGASUS.}
\end{figure}
\footnotetext{Maneuver diagrams taken from www.modelaircraft.org}

\subsection{Aerobatics}

There has also been some later work on using PEGASUS along with Apprenticeship learning in helicopters to do complex tasks like aerobatics \cite{ApprenticeshipHelicopterAerobatics}. Apprenticeship learning provides an efficient method to create a reward function, and PEGASUS learns the policy based on the generated reward function.

To complement this, there has also been work on using PEGASUS for outright unstable states that are difficult for most human pilots. The work by Andrew Ng, Jin Kim, Michael Jordan and Shankar Sastry on Inverted helicopter flight \cite{InvertedHelicopterFlight} uses PEGASUS with an underlying controller. The underlying controller provides a flexibility to the system and generalizes it further.

\begin{figure}[h]
  \centering
    \includegraphics[width=0.5\textwidth]{pegasus_inverted_helicopter.png}
    \caption{Control of a helicopter to hover inverted using Apprenticeship Learning and PEGASUS by Andrew Ng, Jin Kim, Michael Jordan and Shankar Sastry \cite{InvertedHelicopterFlight}.}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Simulation}

As stressed earlier, a simulated environment is the most important to implement PEGASUS. To emulate the quad-rotor, we used ROS \cite{ROS} - The well known open source Robot Operating System along with the physics simulator Gazebo \cite{Gazebo}.

\section{ROS and Gazebo}

ROS and gazebo provide a good API to modify parameters of a simulation, and are intended to provide a software layer on top of hardware. Hence, it is easy to replace the hardware with a simulated hardware. Along with this, it is possible to provide an interface to make controllers with easily configurable time steps and so on.

ROS also provides a method to isolate pieces of code from one another (called nodes) which fits well with the Reinforcement Learning framework where an Agent and Environment need to be isolated. Nodes can then communicate with each other using services or topics.

Topics are intended for continuous stream-like communication at specific intervals. Hence interrupts and streams are easy to create using topics. Topics provide only one way communication. Services provide a method to have 2 way communication. This is similar to calling a function from another node with specific arguments.

Gazebo provides a multi-robot simulation environment. It wraps the ODE and bullet physics engines and provides kinematic and dynamic simulation also. Gazebo is capable of simulating gravity, contact forces, joints and frictions but ignores aerodynamics and propulsion systems which are important factors to consider for a quad-rotor. Gazebo is slow with fluid systems and hence simulating air flow is complicated.

Hence, ROS and Gazebo do provide a good method of simulating

\section{Hector Quadrotor}

Hector Quad-rotor \cite{HectorQuadrotor} is a ROS package which helps to simulate quad-rotors in ROS and Gazebo. It is provided by Team Hector for Urban Search and Rescue (USAR) applications. Hector Quad-rotor was created specifically to path the issues that Gazebo has with simulating UAVs. It gives an easy method to import a quad-rotor model using the COLLADA format (exportable by Blender) while the collision model is modeled as a .stl mesh.

The propellers are modelled as discs as a trade-off between visual accuracy, collision detection and dynamics modeling. Modelling it as fans proved troublesome due to the collision detection which gave varied results with different physics time steps.

The dynamics of the quad-rotor are based on Samir Bouabdallah's work \cite{QuadrotorDynamics}. It includes the flight dynamics, motor dynamics, and thrust calculations required for the system. Sensors have also been emulated using an error model like the Gauss Markov error model. Common sensors like IMU, barometric, ultrasonic, GPS and compass have been implemented in the package.

Hector Quad-rotor also provides a basic controller. The controller is implemented by separate cascaded PID controllers controlling roll and pitch movement, yaw rate and linear z velocity. This assumes that each axis and the altitude can be controlled independently. This is a valid assumption for quad-rotors for moderate deviations from the hovering state.

This controller is abstracted and has implementations to use any sort of input. The types of inputs that are possible are torque (wrench), position (pose), velocity (twist), and also motor velocity. This allows us to easily implement our own controller.

\section{Reinforcement Learning framework}

RL-Glue \cite{RLGlue} is a well known Reinforcement Learning framework which helps to write Agents and Environments that are independent of each other. RL-Glue works by using sockets which is very similar to how ROS nodes are implemented. RL-Glue has a thin wrapper for ROS called ROS-Glue, but this has become unsupported and is old.

Hester T. and Stone P. made a ROS package for their paper on TEXPLORE \cite{Texplore}. The rl-texplore-ros-pkg\footnote{https://github.com/toddhester/rl-texplore-ros-pkg} provides topics and messages which help in communicating between an agent and an environment. It can also handle multiple agents and has some popular algorithms already implemented.

This package however did not integrate gazebo into the environment, and we made our own package\footnote{https://github.com/AbdealiJK/quadrotor\_control} which could do this. The texplore package also did not implement Pegasus. This modified package is able to communicate with gazebo as part of the environment and can use the models of the gazebo simulator and converts it into a state. It also has the ability to run gazebo in finite time steps, pausing the simulator between them.

\subsection*{Agent}

The agent is a ROS node which implements from the abstract class provided in the texplore package \texttt{Agent}. It implements the functions first action, last action and next action which are intended to initiate the episode, end the episode and continue to the next time step of the episode respectively.

The Agent automatically transmits the action using the RLAction message on the topic \texttt{rl\_agent/rl\_action}. The message is an array of any length of float32 type. Hence, it is possible to supply any number of actions for a complex agent.

\subsection*{Environment}

The environment is a ROS node which implements from the abstract class provided in texplore package \texttt{Environment}. It implements the function sensation, apply, terminal, and reset. The function sensation is the state or the observation from the environment that the agent is capable of observing. The function apply takes in the action given by an agent and applies that to the environment. The terminal and reset functions act as destructors and constructors to reuse the environment in various episodes.

The environment transmits the state information using the RLEnv message on the topic \texttt{rl\_env/rl\_state\_reward}. The message has 3 variables. An array of any length which has float32 items which is the state or the observable state by the agent. A scalar of type float32 which is the reward obtained by the last step (In the start state, this is ignored). And finally a boolean which indicates whether the state is a terminal state or not.

An environment named Hector Quad-rotor has been implemented which passes the action from the agent to the gazebo environment as a twist state. The sensation function is also implemented in such a way as to wait for gazebo to complete a transition step and return back the observable state from this. For all this a random seed can be set in gazebo to create the repeatable scenarios required by PEGASUS.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Quad-rotor dynamics}

Quad-rotor dynamics can be controlled by controlling the rotor thrusts. This is used to change the altitude, pitch, roll, and yaw of the quad-rotor. We can compute how the rotor thrust affects the roll, pitch, yaw, and altitude using the model of the quad-rotor. Using this, we analyze how a quad-rotor is capable of moving in 6 degrees of freedom (linear directions and the Euler angles) with only 4 controllable inputs.

Combining the revolutions per minute (rpm) of the 4 rotors makes it possible for the quad-rotor to turn, and move in various ways. The following configurations are possible:

\todo{Add picture of quad from top with names for the rotors. (Check the rotor names from wikipedia https://en.wikipedia.org/wiki/Quadcopter}

{\bf Altitude (z)}: To change altitude (in the positive z direction) equal thrust is applied to all the rotors. Alternate rotors are given thrust in opposite directions so that the torque does not cause the quad-rotor to change it's pitch and roll. For example, if Rotors 1 and 3 are moving in the clockwise direction Rotors 2 and 4 need to move in anti-clockwise direction. This is also required for hovering as there is a constant gravitational force acting downward which needs to be countered.

{\bf Yaw}: To change the yaw while hovering, the thrust applied to the rotors need to be different so that torque is not zero. But as we do not want a force in the z direction, alternate rotors have the same thrust. For example, Rotors 1 and 3 need the same thrust and Rotors 2 and 4 also need the same thrust. But one pair will have a lesser thrust than the other depending on the direction in which yaw needs to be changed.

{\bf Pitch and Roll}: In a quad-rotor, it can be observed that the x and y directions (or the pitch and roll angles) are symmetric. This symmetric nature let's us use the similar control logic for both these directions. To change the pitch and roll, the alternate motors move in opposite directions to counter torque appropriately. Here, one pair of diametrically opposite motors have the same thrust while the other pair does not. This causes the quad-rotor to turn toward the direction of the lesser rotor. For example, if Rotors 2 and 4 have the same thrust, Rotor 1 would have more thrust than Rotor 3 (or vice-versa). Similarly, if Rotors 1 and 3 have the same thrust the quad-rotor would move in the other symmetric direction.

\todo{Add subfigures of picture of quad from top with direction of motor rpm. (Check from wikipedia https://en.wikipedia.org/wiki/Quadcopter)}

\section{System modelling}

The quad-rotor dynamics have been documented by Samir Boubdallah and Roland Siegwart for the OS4 project \cite{QuadrotorDynamics}. This is the same modelling that is used in the Hector Quad-rotor package.

In the paper, each part of the quad-rotor is modelled, starting from the motors. The motor is taken to be a simple DC motor where the thrust $T$ is proportional to the vertical force acting on all the blade elements. The horizontal force (called Hub Force $H$ is found by calculating the resultant horizontal force and the drag $Q$. is found using a similar method.

Once the basic linear forces have been accounter for, the angular forces are found. The angular forces include the Rolling moments, Pitching moments, and Yawing moments. They go on to find the dynamic equation of the BLDC rotors also, as this would contribute to the moments to a small extent. This is found using a simple first order transfer function - to recreate the dynamics the propeller would acheive between the set-point required and the actual speed.

Using these individual forces, the net force on the quad-rotor can be calculated. This gives a set of equations defining the moments and forces (See equation 7 in Samir Boubdallah and Roland Siegwart's work \cite{QuadrotorDynamics}). Using this, it is now possible to define the the transition function as:

\begin{equation} \begin{split}
  f(\bar{S}, \bar{U}) = \left\{
    \begin{array}{c}
      \dot{\phi} \\
      \dot{\theta} \dot{\psi} a_{1} + \dot{\theta} a_{2} \omega_{r} + b_{1} U_{2} \\
      \dot{\theta} \\
      \dot{\phi} \dot{\psi} a_{1} + \dot{\phi} a_{4} \omega_{r} + b_{2} U_{3} \\
      \dot{\psi} \\
      \dot{\theta} \dot{\phi} a_{5} + b_{3} U_{4} \\
      \dot{z} \\
      g  - (\cos{\phi} \cos{\theta})\frac{U_{1}}{m} \\
      \dot{x} \\
      u_{x} \frac{U_{1}}{m} \\
      \dot{y} \\
      u_{y} \frac{U_{1}}{m}
    \end{array}
  \right\}
\end{split} \end{equation}

Where $\bar{U}$ is the action which is a 4 dimensional vector (1 action per rotor). The constants $m$, $a_{i}$, and $b_{i}$ are system parameters that can be calculated from properties like moment of inertia, mass, etc. $u_{j}$ is the projection from Spherical co-ordinates to Cartesian of a unit vector in the $j$ dimension in Cartesian co-ordinates.

This model is useful, as it helps to formulate the best policy class that can be used for this system in the case of PEGASUS.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Position control}

As described earlier, velocity control has already been implemented in the Hector Quadrotor package. This is simply an abstraction from the motor to velocity of the quad-rotor itself. This is deterministic and is based on the model information of the quad-rotor. No learning takes place here.

On top of velocity, the next step is to establish a position control using PEGASUS. Here, when doing the learning, we first train the quad-rotor to learn by itself. Then we use the learnt policy and fine tune it for a particular payload. This benefit of this system is that the learning for the payload is much faster because the initial policy there is close to the optimal policy.

Hence, we again abstract the payload out from the learning. This enables us to train a model to learn multiple different payloads faster. And we transfer the knowledge learnt from the quad-rotor easily without excessive learning.

\section{Quad-rotor control}

\todo{Add picture of quadrotor from simulations}

A typical implementation of PEGASUS in our case would have atleast 4 paramters in our policy class (for roll, pitch, yaw, and altitude) that need to be trained. We used a gradient descent method to find the maimum valued policy in the PEGASUS algorithm.

The {\bf state} has 8 terms, of which 4 are independent (they can be calculated from the other 4). It captures the absolute position of the robot from a given origin. This can be found using external sensors like a set of cameras pointing a the quad-rotor or onboard sensors like IMU and altimeter. The state vector is:
\begin{equation}
  State = s = [x, y, z, \omega, \dot{x}, \dot{y}, \dot{z}, \dot{w}]
\end{equation}
Where, $(x, y, z)$ is the position of the center of mass of the quad-rotor at any given time, and $\omega$ is the yaw of the quad-rotor. The state could potentially also include the roll and pitch of the quad-rotor - but we found that no significant improvement was found using those parameters. Also, theoretically the pitch and roll are correlated to the velocity $(\dot{x}, \dot{y}, \dot{z})$ and so offer no new data to learn using.

For the origin, we use the relative position of the quad-rotor from the target position. This allows us to not be constrained in a specific area in space. This is bostered by the intuitive observation that the policy wouldn't change based on absolute position, rather it would change based on the relative position of the target and the robot. This reduces learing time drastically and makes the polciy generalized over space.

The {\bf action} used in our implementation are the abstracted velocities after the velocity control. The action vector is:
\begin{equation}
  Action = a = [\dot{x}, \dot{y}, \dot{z}, \dot{\omega}]
\end{equation}
The action used is the abstract velocity, as this allows us to be flexible when choosing the system. This same PEGASUS formulation can be used if the robot is an octa-quad or similar UAVs, without any change in the position control. The benefit here, is that if a multi rotor UAV finds that there is a defect in one of it's motors, it is easy to switch over to another policy which does not use this motor. This enables the UAV to be smarter and control damage without affecting performance.

The {\bf policy} class used for the PEGASUS is from the above mentioned state space to the action space $\pi: S \rightarrow A$. The policy is written as:
\begin{equation}
  Policy = \pi(s) = \left\{
    \begin{array}{c}
      c_1 \times x + c_2 \times \dot{x} \\
      c_3 \times y + c_4 \times \dot{y} \\
      c_5 \times z + c_6 \times \dot{z} \\
      c_7 \times \omega + c_8 \times \dot{\omega} \\
    \end{array}
  \right\}
\end{equation}


The {\bf reward} used is the L1 norm of the deviation from the target. The reward is negative, and so the maximum reward that the quad-rotor can acheive is zero. A zero reward is got when it hovers exactly at the target. The reward function is written as:
\begin{equation}
  Reward = r(s) = - |x - x_0| - |y - y_0| - |z - z_0| - K \times |\omega - \omega_0|
\end{equation}

The roll and pitch are not considered in the target nor the state as a quadrotor cannot hover at a position with a non-zero roll and pitch. Hence, we alway assume the target roll and pitch to be zero. It is impossible to learn a policy which allows the quadrotor to maintain a specific roll and pitch as it would simply move to another orientation, and when returning back to the $(x, y, z, \omega)$ the roll and pitch will again change.

Trying to maintain a roll and pitch is the specific case of unstable targets. In aerobatics, veteran human pilots attempt to maintain the quadrotor in an inverted state. Our method gives the quad-rotor a negative reward in such cases as this is a difficult method to learn. The policy is unable to handle a unstable like this, and things spiral out of control.

Unstable states like the ones seen in aerobatics have the problem that the mechanism is too complex to model into simple closed form reward functions. Apprenticeship learning is something that would make this possible, as was done by Andrew Ng, Jin Kim, Michael Jordan, and Shankar Sastry in their work with inverted helicopters \cite{InvertedHelicopterFlight}.

\todo{Add graph of value of the policy being found over time}

\todo{Add 3d schematic of it going to a position with path trace}

\todo{Add 3d schenatic of it going to a position with path trace and angle for yaw}

\todo{Add 3d schematic of learning with path trace}

\subsection{Time analysis}

The gradient descent method attempts to move in small steps in various directions and approximates the gradient of this multi-dimensional function. Hence, if there are $P$ parameters in the policy class, it calcuates the value of the policy $2 \times P$ times. Now if we run the gradient descent $T$ times, the number of episodes that are run are $2 \times H \times P \times T$ where $H$ is the number of times we average the value of the policy in 1 scenario.

We notice that a few parameters are independent in the chosen policy class. For example, $c_1$ and $c_2$ can be learnt independent of $c_7$ and $c_8$. Hence by learning this piece by piece, we can increase the chance of converging faster. This basically means we would be running multiple policy gradient on various parameters independently.

Currently, ODE does not have GPU capabilities and so is forced to run in the CPU. Because of the nature of physics simulations (where every time step depends on the last) it is not possible to parallelize the steps. Running gradient descent in smaller subsets of the policy also makes it possible to parallelize the learning over multiple computers, and hence faster.

\todo{Add graph of value of the policy being found over time (compare with earlier time)}

\todo{Add 3d schematic of learning with path trace for each component}

\section{Payload control}

\todo{Add picture of quadrotor with payload from simulations}

The next step is to control the payload's position. Here, we add a payload with some freedom at the joint. This makes the payload move in it's own path, not fixed by the quad-rotor's path. Hence, the inertia the payload accumulates cannot be directly controlled.

The state vector used in PEGASUS is again the same as above, but it uses the state of the payload rather than that of the quad-rotor. Also, the target state that needs to be acheived is the payload's state.

The action is still the action of the quadrotor. There is no action that can be done directly on the payload as the joint is not actuated. Hence, the only control inputs that we have is the velocity of the quadrotor. It is interesting to note that when using the above reward and policy class the payload moves in a smooth trajectory without much deviation from the intended behaviour. This is without any state information of the quad-rotor. This is probably because the state information of the quadrotor is highly correlated to the state of the payload. If we wish to add more joints (like a chain for example), this would not hold true, and a better policy would need to be considered.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Trajectory control}

Once the velocity and position control are established, we would want the payload and the quad-rotor or payload to move in trajectories. A trajectory is a spatial function which relates position and time. Hence, we now try to control the position the quad-rotor or payload attains at a specific time, introducing the new variable time.

There are multiple different methods to trace a trajectory after position or velocity control are established. Each technique has it's strengths and weaknesses. So we analyze three methods which use a pre existing trajectory which is given before hand. This trajectory information is then used to find the optimal controls to give to the quad-rotor to acheive the trajectory.

\section{Predator Prey}

In predator prey, the complete trajectory function with time is known. Here, the target of the quad-rotor is changed at every step to be the expected position after a certain time. Hence, we can say the quad-rotor or payload (predator) tries to pursue the trajectory (prey) where the target has a time lead.

There has been some work in predator prey methods \todo{Add more info and papers}

\todo{Add figure explaining how predator-prey works}

\section{Checkpoint}

A checkpoint is defined as a set of points on the trajectory. The quad-rotor or payload is expected to go through these checkpoints in the appropriate order. Hence, this is anologous to the quad-rotor going through loops which are predefined in an arena. The checkpoints do not have specific timestamps here, but there is an ordering that needs to be followed.

The quad-rotor or payload is considered to have gone through a checkpoint if it moved "close" to that point. For example, consider a checkpoint that is defined by the position information:
\begin{equation}
  [x_c, y_c, z_y, \omega_c]
\end{equation}
We consider the quad-rotor or payload to have gone through this checkpoint if their position is within a spatial neighbourhood of the checkpoint. This can be written as:
\begin{equation} \begin{split}
  |x - x_c| &< \epsilon_x \\
  |y - y_c| &< \epsilon_y \\
  |z - z_c| &< \epsilon_z \\
  |\omega - \omega_c| &< \epsilon_\omega
\end{split} \end{equation}
The $\epsilon_i$ would be different for $\omega$ as compared to the others because of the units. Also, we notice that $\epsilon_z$ need not be very small as altitude control of the quadrotor is considerably better than x or y. Larger errors in altitude can be easily corrected when prusuing the next checkpoint.

Some noteworthy comments about this technique are:

{\bf Retracing paths}: If a checkpoint is missed due to control error in the trajectory, the quad-rotor would be forced to go backward and find this checkpoint. This is counter productive and may not be a good idea because it modifies the original structure of the trajectory that had to be traced.

{\bf Hard bounds on $\epsilon_i$}: The $\epsilon_i$ which is used to check whether the checkpoint was passed is a hard bound. For example, if the quad-rotor or payload is a little outside this boundary, the checkpoint is not passed. This boundary is arbitrary and can be made soft using fuzzy concepts.

{\bf Jerky trajectories}: The underlying policy that is being used is the position control. So the quad-rotor thinks that it should stop at the next checkpoint because it has reached the target. Hence, it decelerate when it is in the checkpoint's vicinity. This causes the trajectory to be a little jerky adding excessive noise near the checkpoint.

{\bf Piecewise smooth}: Although the trajectory is jerky near the checkpoint, we can term this trajectory as piecewise smooth. Between checkpoints, the trajectory is very smooth and the expected path that the original trajectory that the checkpoints were generated from.

{\bf Generating checkpoints}: There is no direct method to generate checkpoints. It is important that checkpoints are chosed with an appropriate distance between them. Intuitively we notice that in a smooth part of the trajectory few checkpoints are needed. But near a sharp turn, more checkpoints are required so as to decelerate appropriately and make a fine turn.

\todo{Add figure explaining what the checkpoint method does}

\section{Waypoint}

A waypoint is similar to the checkpoint concept. We can think of this as a special checkpoint where the condition to pass is that the quad-rotor or payload passes the plane defined by the normal to the trajectory at this point. The quad-rotor or payload can also be restricted to pass through a specific patch in the plane. Hence, we are not restricted to the absolute $(x, y, z)$ directions, rather the frame of the expected position is used.

Because we simply need the quadrotor to pass the plane of the waypoint, even if the control is bad at certain areas, it does not change the original structure of the trajectory. This comes at the cost that the error in this direction could increase over time. However, we expect that this would not be unbounded as the quad-rotor policy would not allow this unless there is a flaw in the controller or the policy.

We notice that we have similar properties to the checkpoint technique, but this solves the issue of unwanted retracing of paths. We again find a fuzzy boundary for $\epsilon_i$ would prove better. But a fuzzy waypoint would give the benefit that the $\epsilon_i$ boundary is in the frame of the trajectory itself. This enables us to have a finer control of the structure of the expected position. Consider the case where the expected trajectory is a circle. If an oval is an acceptable trajectory instead of a circle, a waypoint method (or a fuzzy waypoint) would be able to acheive that considerably easier. This is because the boundaries are in the frame of the waypoint, and this changes at every instant in a circle.

\todo{Add figure explaining what the waypoint method does}


\todo{Add figure showing the path it follows with these 3 tecnhiques}

\todo{Add figure showing the deviation over time for a circle for all 3 techniques}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Apprenticeship Learning}

\todo{}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Bibliography.

\begin{singlespace}
  \bibliography{refs}
\end{singlespace}

\end{document}
